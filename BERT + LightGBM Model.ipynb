{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f44662fb",
   "metadata": {},
   "source": [
    "# BERT + LightGBM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e1e407",
   "metadata": {},
   "source": [
    "### Proposed Model\n",
    "\n",
    "Used BERT embeddings for LightGBM classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0042014",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\manas\\anaconda3\\anaconda\\Lib\\site-packages\\transformers\\optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Training Epoch 1: 100%|██████████████████████████████████████████████████████████████| 132/132 [07:13<00:00,  3.29s/it]\n",
      "Training Epoch 2: 100%|██████████████████████████████████████████████████████████████| 132/132 [06:46<00:00,  3.08s/it]\n",
      "Training Epoch 3: 100%|██████████████████████████████████████████████████████████████| 132/132 [06:42<00:00,  3.05s/it]\n",
      "Training Epoch 4: 100%|██████████████████████████████████████████████████████████████| 132/132 [06:44<00:00,  3.06s/it]\n",
      "Extracting BERT embeddings: 100%|██████████████████████████████████████████████████| 1056/1056 [01:03<00:00, 16.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Info] Number of positive: 499, number of negative: 345\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022720 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195822\n",
      "[LightGBM] [Info] Number of data points in the train set: 844, number of used features: 768\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.591232 -> initscore=0.369062\n",
      "[LightGBM] [Info] Start training from score 0.369062\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Best parameters found by GridSearchCV: {'feature_fraction': 0.8, 'learning_rate': 0.01, 'n_estimators': 200, 'num_leaves': 31}\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 499, number of negative: 345\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018006 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195822\n",
      "[LightGBM] [Info] Number of data points in the train set: 844, number of used features: 768\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.591232 -> initscore=0.369062\n",
      "[LightGBM] [Info] Start training from score 0.369062\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "Accuracy: 0.9858\n",
      "Precision: 0.9841\n",
      "Recall: 0.9920\n",
      "F1 Score: 0.9880\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, AdamW, get_linear_schedule_with_warmup, BertModel\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Load Dataset\n",
    "true_data = pd.read_csv('politifact_real.csv')\n",
    "fake_data = pd.read_csv('politifact_fake.csv')\n",
    "\n",
    "# Preprocess the text data\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('\\n', ' ').replace('\\r', '').replace('\\t', ' ')\n",
    "    text = ''.join([char for char in text if char.isalnum() or char in [' ', \"'\"]])\n",
    "    return text\n",
    "\n",
    "true_data['title'] = true_data['title'].apply(preprocess_text)\n",
    "fake_data['title'] = fake_data['title'].apply(preprocess_text)\n",
    "\n",
    "# Generate labels True/Fake under new Target Column in 'true_data' and 'fake_data'\n",
    "true_data['Target'] = ['True'] * len(true_data)\n",
    "fake_data['Target'] = ['Fake'] * len(fake_data)\n",
    "\n",
    "# Merge 'true_data' and 'fake_data', by random mixing into a single df called 'data'\n",
    "fake_news_data = pd.concat([true_data, fake_data]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Load BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Tokenize and encode the dataset\n",
    "def encode_data(text_list):\n",
    "    encoded_inputs = tokenizer(text_list, padding=True, truncation=True, max_length=256, return_tensors='pt')\n",
    "    return encoded_inputs\n",
    "\n",
    "# Fine-tune the BERT model\n",
    "def fine_tune_bert(model, data, labels, epochs=4, batch_size=8):\n",
    "    inputs = encode_data(data)\n",
    "    dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'], torch.tensor(labels))\n",
    "    dataloader = DataLoader(dataset, sampler=RandomSampler(dataset), batch_size=batch_size)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "    total_steps = len(dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch in tqdm(dataloader, desc=\"Training Epoch {}\".format(epoch+1)):\n",
    "            batch_input_ids, batch_attention_mask, batch_labels = batch\n",
    "            model.zero_grad()\n",
    "            outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask, labels=batch_labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "# Prepare data for fine-tuning\n",
    "labels = fake_news_data['Target'].apply(lambda x: 1 if x == 'True' else 0).values\n",
    "fine_tune_bert(model, fake_news_data['title'].tolist(), labels)\n",
    "\n",
    "# Extract BERT embeddings after fine-tuning\n",
    "def get_bert_embeddings(data):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(data, desc=\"Extracting BERT embeddings\"):\n",
    "            encoded_inputs = encode_data([text])\n",
    "            outputs = model.bert(**encoded_inputs)\n",
    "            embeddings.append(outputs.last_hidden_state[:, 0, :].numpy())\n",
    "    return np.concatenate(embeddings, axis=0)\n",
    "\n",
    "# Get embeddings for the titles\n",
    "titles = fake_news_data['title'].tolist()\n",
    "embeddings = get_bert_embeddings(titles)\n",
    "\n",
    "# Prepare data for LightGBM\n",
    "X = embeddings\n",
    "y = labels\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'num_leaves': [31, 50, 70],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'feature_fraction': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "# Initialize LightGBM model\n",
    "lgbm = lgb.LGBMClassifier(objective='binary', metric='binary_logloss', boosting='gbdt')\n",
    "\n",
    "# Setup GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=lgbm, param_grid=param_grid, cv=KFold(n_splits=3), scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best parameters found by GridSearchCV:\", best_params)\n",
    "\n",
    "# Train the final model with best parameters\n",
    "best_lgbm = lgb.LGBMClassifier(**best_params)\n",
    "best_lgbm.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = best_lgbm.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7631567a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
