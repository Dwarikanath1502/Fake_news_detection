{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f44662fb",
   "metadata": {},
   "source": [
    "# BERT + LightGBM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84484323",
   "metadata": {},
   "source": [
    "### Proposed Model\n",
    "\n",
    "Used BERT embeddings for LightGBM classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c9447c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manas\\AppData\\Local\\Temp\\ipykernel_12460\\3343559130.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(input_ids)\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\manas\\anaconda3\\anaconda\\Lib\\site-packages\\transformers\\optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Training loss: 0.4095031751272696\n",
      "Accuracy: 0.8525293586269196\n",
      "Precision: 0.7860635696821516\n",
      "Recall: 0.5735950044603033\n",
      "F1-score: 0.6632284682826199\n",
      "\n",
      "\n",
      "Epoch 2/3\n",
      "Training loss: 0.3005922428443221\n",
      "Accuracy: 0.8604336043360433\n",
      "Precision: 0.7880870561282932\n",
      "Recall: 0.6137377341659233\n",
      "F1-score: 0.6900702106318957\n",
      "\n",
      "\n",
      "Epoch 3/3\n",
      "Training loss: 0.21826962339910358\n",
      "Accuracy: 0.8556910569105691\n",
      "Precision: 0.7510416666666667\n",
      "Recall: 0.6431757359500446\n",
      "F1-score: 0.6929360884190293\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load Dataset\n",
    "true_data = pd.read_csv('gossipcop_real.csv')\n",
    "fake_data = pd.read_csv('gossipcop_fake.csv')\n",
    "\n",
    "# Preprocess the text data\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('\\n', ' ').replace('\\r', '').replace('\\t', ' ')\n",
    "    text = ''.join([char for char in text if char.isalnum() or char in [' ', \"'\"]])\n",
    "    return text\n",
    "\n",
    "true_data['title'] = true_data['title'].apply(preprocess_text)\n",
    "fake_data['title'] = fake_data['title'].apply(preprocess_text)\n",
    "\n",
    "# Generate labels True/Fake under new Target Column in 'true_data' and 'fake_data'\n",
    "true_data['Target'] = ['True'] * len(true_data)\n",
    "fake_data['Target'] = ['Fake'] * len(fake_data)\n",
    "\n",
    "# Merge 'true_data' and 'fake_data', by random mixing into a single df called 'data'\n",
    "fake_news_data = pd.concat([true_data, fake_data]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize input text and pad/truncate sequences\n",
    "max_length = 128  # Maximum sequence length\n",
    "tokenized_texts = [tokenizer.encode(text, add_special_tokens=True, max_length=max_length, truncation=True) for text in fake_news_data['title']]\n",
    "\n",
    "# Pad sequences\n",
    "input_ids = torch.nn.utils.rnn.pad_sequence([torch.tensor(tokenized_text) for tokenized_text in tokenized_texts], batch_first=True)\n",
    "\n",
    "# Create attention masks\n",
    "attention_masks = [[int(token_id > 0) for token_id in input_id] for input_id in input_ids]\n",
    "\n",
    "# Prepare Labels\n",
    "labels = [1 if label == 'Fake' else 0 for label in fake_news_data['Target']]\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "input_ids = torch.tensor(input_ids)\n",
    "attention_masks = torch.tensor(attention_masks)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_inputs, test_inputs, train_masks, test_masks, train_labels, test_labels = train_test_split(input_ids, attention_masks, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define BERT model for sequence classification with fewer layers\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=2,  \n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    "    num_hidden_layers=6  # Set the number of layers (default is 12)\n",
    ")\n",
    "\n",
    "# Fine-tuning BERT model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer and learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "epochs = 3\n",
    "total_steps = len(train_inputs) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Data loaders\n",
    "batch_size = 32\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        input_ids, masks, labels = tuple(t.to(device) for t in batch)\n",
    "        model.zero_grad()\n",
    "        outputs = model(input_ids, token_type_ids=None, attention_mask=masks, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Clip gradients to avoid exploding gradients\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    predictions, true_labels = [], []\n",
    "\n",
    "    for batch in test_dataloader:\n",
    "        input_ids, masks, labels = tuple(t.to(device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, token_type_ids=None, attention_mask=masks)\n",
    "            logits = outputs.logits\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = labels.to('cpu').numpy()\n",
    "\n",
    "        predictions.append(logits)\n",
    "        true_labels.append(label_ids)\n",
    "\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_labels = np.concatenate(true_labels, axis=0)\n",
    "    preds_flat = np.argmax(predictions, axis=1).flatten()\n",
    "    labels_flat = true_labels.flatten()\n",
    "    accuracy = accuracy_score(labels_flat, preds_flat)\n",
    "    precision = precision_score(labels_flat, preds_flat)\n",
    "    recall = recall_score(labels_flat, preds_flat)\n",
    "    f1 = f1_score(labels_flat, preds_flat)\n",
    "    print(f'Epoch {epoch + 1}/{epochs}')\n",
    "    print(f'Training loss: {avg_train_loss}')\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'F1-score: {f1}')\n",
    "    print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b9ccc4",
   "metadata": {},
   "source": [
    "### Another Proposed Model\n",
    "\n",
    "BERT and LightGBM works seperately of each other and then we combine their two predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7368e578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from lightgbm import LGBMClassifier\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load Dataset\n",
    "true_data = pd.read_csv('gossipcop_real.csv')\n",
    "fake_data = pd.read_csv('gossipcop_fake.csv')\n",
    "\n",
    "# Generate labels True/Fake under new Target Column in 'true_data' and 'fake_data'\n",
    "true_data['Target'] = ['True']*len(true_data)\n",
    "fake_data['Target'] = ['Fake']*len(fake_data)\n",
    "\n",
    "# Merge 'true_data' and 'fake_data', by random mixing into a single df called 'data'\n",
    "fake_news_data = pd.concat([true_data, fake_data]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Preprocess the text data using TF-IDF vectorization for LightGBM\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_tfidf = vectorizer.fit_transform(fake_news_data['title']).toarray()\n",
    "y_tfidf = fake_news_data['Target']\n",
    "\n",
    "# Split the TF-IDF dataset into training and testing sets\n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(X_tfidf, y_tfidf, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the LightGBM Classifier\n",
    "lgbm_classifier = LGBMClassifier()\n",
    "lgbm_classifier.fit(X_train_tfidf, y_train_tfidf)\n",
    "\n",
    "# Tokenize and encode the training data for BERT\n",
    "X_train, X_test, y_train, y_test = train_test_split(fake_news_data['title'], fake_news_data['Target'], test_size=0.2, random_state=42)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "X_train_encoded = tokenizer(X_train.tolist(), padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "y_train_encoded = torch.tensor([1 if label == 'True' else 0 for label in y_train.tolist()])\n",
    "\n",
    "# Fine-tune BERT on the training data\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(**X_train_encoded, labels=y_train_encoded)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Make predictions using LightGBM\n",
    "y_pred_lgbm = lgbm_classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Make predictions using BERT\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X_test_encoded = tokenizer(X_test.tolist(), padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "    outputs = model(**X_test_encoded)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=1)\n",
    "y_pred_bert = ['True' if pred == 1 else 'Fake' for pred in predictions.tolist()]\n",
    "\n",
    "# Combine predictions\n",
    "combined_predictions = []\n",
    "for pred_lgbm, pred_bert in zip(y_pred_lgbm, y_pred_bert):\n",
    "    if pred_lgbm == pred_bert:\n",
    "        combined_predictions.append(pred_lgbm)\n",
    "    else:\n",
    "        combined_predictions.append(pred_lgbm)  # You may want to use a different strategy for combining predictions\n",
    "\n",
    "# Evaluate the combined predictions\n",
    "accuracy_combined = accuracy_score(y_test_tfidf, combined_predictions)\n",
    "precision_combined = precision_score(y_test_tfidf, combined_predictions, average='weighted')\n",
    "recall_combined = recall_score(y_test_tfidf, combined_predictions, average='weighted')\n",
    "f1_combined = f1_score(y_test_tfidf, combined_predictions, average='weighted')\n",
    "\n",
    "# Print evaluation metrics for the combined predictions\n",
    "print(\"Combined Classifier:\")\n",
    "print(\"Accuracy:\", accuracy_combined)\n",
    "print(\"Precision:\", precision_combined)\n",
    "print(\"Recall:\", recall_combined)\n",
    "print(\"F1 Score:\", f1_combined)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
