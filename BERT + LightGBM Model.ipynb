{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f44662fb",
   "metadata": {},
   "source": [
    "# BERT + LightGBM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84484323",
   "metadata": {},
   "source": [
    "### Proposed Model\n",
    "\n",
    "Used BERT embeddings for LightGBM classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c9447c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split, GridSearchCV\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load Dataset\n",
    "true_data = pd.read_csv('gossipcop_real.csv')\n",
    "fake_data = pd.read_csv('gossipcop_fake.csv')\n",
    "\n",
    "# Preprocess the text data\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('\\n', ' ').replace('\\r', '').replace('\\t', ' ')\n",
    "    text = ''.join([char for char in text if char.isalnum() or char in [' ', \"'\"]])\n",
    "    return text\n",
    "\n",
    "true_data['title'] = true_data['title'].apply(preprocess_text)\n",
    "fake_data['title'] = fake_data['title'].apply(preprocess_text)\n",
    "\n",
    "# Generate labels True/Fake under new Target Column in 'true_data' and 'fake_data'\n",
    "true_data['Target'] = ['True'] * len(true_data)\n",
    "fake_data['Target'] = ['Fake'] * len(fake_data)\n",
    "\n",
    "# Merge 'true_data' and 'fake_data', by random mixing into a single df called 'data'\n",
    "fake_news_data = pd.concat([true_data, fake_data]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Tokenize and encode the dataset\n",
    "def encode_data(text_list):\n",
    "    encoded_inputs = tokenizer(text_list, padding=True, truncation=True, max_length=256, return_tensors='pt')\n",
    "    return encoded_inputs\n",
    "\n",
    "# Fine-tune the BERT model\n",
    "def fine_tune_bert(model, data, labels, epochs=4, batch_size=8):\n",
    "    inputs = encode_data(data)\n",
    "    dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'], torch.tensor(labels))\n",
    "    dataloader = DataLoader(dataset, sampler=RandomSampler(dataset), batch_size=batch_size)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "    total_steps = len(dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch in tqdm(dataloader, desc=\"Training Epoch {}\".format(epoch+1)):\n",
    "            batch_input_ids, batch_attention_mask, batch_labels = batch\n",
    "            model.zero_grad()\n",
    "            outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask, labels=batch_labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "# Prepare data for fine-tuning\n",
    "labels = fake_news_data['Target'].apply(lambda x: 1 if x == 'True' else 0).values\n",
    "fine_tune_bert(model, fake_news_data['title'].tolist(), labels)\n",
    "\n",
    "# Extract BERT embeddings after fine-tuning\n",
    "def get_bert_embeddings(data):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(data, desc=\"Extracting BERT embeddings\"):\n",
    "            encoded_inputs = encode_data([text])\n",
    "            outputs = model.bert(**encoded_inputs)\n",
    "            embeddings.append(outputs.last_hidden_state[:, 0, :].numpy())\n",
    "    return np.concatenate(embeddings, axis=0)\n",
    "\n",
    "# Get embeddings for the titles\n",
    "titles = fake_news_data['title'].tolist()\n",
    "embeddings = get_bert_embeddings(titles)\n",
    "\n",
    "# Prepare data for LightGBM\n",
    "X = embeddings\n",
    "y = labels\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'num_leaves': [31, 50, 70],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'feature_fraction': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "# Initialize LightGBM model\n",
    "lgbm = lgb.LGBMClassifier(objective='binary', metric='binary_logloss', boosting='gbdt')\n",
    "\n",
    "# Setup GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=lgbm, param_grid=param_grid, cv=KFold(n_splits=3), scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best parameters found by GridSearchCV:\", best_params)\n",
    "\n",
    "# Train the final model with best parameters\n",
    "best_lgbm = lgb.LGBMClassifier(**best_params)\n",
    "best_lgbm.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = best_lgbm.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
